{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3740a11-70c9-4c43-abcd-86f4ba96f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "from llama_index.core.ingestion import (\n",
    "    DocstoreStrategy,\n",
    "    IngestionPipeline,\n",
    "    IngestionCache,\n",
    ")\n",
    "# from llama_index.core import (\n",
    "#     SimpleDirectoryReader,\n",
    "#     load_index_from_storage,\n",
    "#     VectorStoreIndex,\n",
    "#     StorageContext,\n",
    "# )\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "import faiss\n",
    "\n",
    "# dimensions of text-ada-embedding-002\n",
    "d = 384 #1536\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "\n",
    "# load_dotenv() \n",
    "# GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  \n",
    "\n",
    "# llm = Groq(model=\"llama3-70b-8192\", api_key=GROQ_API_KEY)\n",
    "\n",
    "# embed_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
    "# define your custom embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\"Data\").load_data()\n",
    "\n",
    "# # now you can use this service context when creating your index\n",
    "# index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "\n",
    "#vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "# create a FAISS vector store\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        embed_model,\n",
    "    ],\n",
    "    \n",
    "    docstore=docstore,\n",
    "    vector_store=vector_store,\n",
    "    docstore_strategy=DocstoreStrategy.UPSERTS,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4332a304-932a-4e92-ac71-0e1496075e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bbfd66d0-e7e4-4dad-a2b2-5596159fd6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 13 Nodes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nodes = pipeline.run(documents=documents)\n",
    "print(f\"Ingested {len(nodes)} Nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0cf38d7a-8c6e-429c-a9aa-41742eb1ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Settings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from llama_index.llms.groq import Groq\n",
    "load_dotenv() \n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  \n",
    "\n",
    "llm = Groq(model=\"llama3-70b-8192\", api_key=\"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\")\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(llm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53bb4a12-1897-4981-a1ab-798d4f1a3b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no relevant information in the provided context to answer the query \"Before college\". The context appears to be related to multimodal models, AI, and programming, but it does not mention anything about college or a time period before college."
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(streaming=True)\n",
    "streaming_response = query_engine.query(\"What is an agent?\")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4ecc0d0-ac56-4a7f-95eb-02fe22b1a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363766f-07f0-4778-a226-bf98f461f00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b06d1-06f8-442a-879b-f0fa08c12241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568e1f2-bdc2-422b-b096-a263778fdcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209f485-067c-46d1-9bcc-5a3e85dc52cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
