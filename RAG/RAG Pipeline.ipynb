{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d75c74d3-a7ba-4359-b6bf-71d2f614e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.ingestion import (\n",
    "    DocstoreStrategy,\n",
    "    IngestionPipeline,\n",
    "    IngestionCache,\n",
    ")\n",
    "from llama_index.storage.kvstore.redis import RedisKVStore as RedisCache\n",
    "from llama_index.storage.docstore.redis import RedisDocumentStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.redis import RedisVectorStore\n",
    "\n",
    "from redisvl.schema import IndexSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2742d067-ae62-4363-a9ab-f3debb2416c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:48:17 redisvl.index.index INFO   Index already exists, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "custom_schema = IndexSchema.from_dict(\n",
    "    {\n",
    "        \"index\": {\"name\": \"gdrive\", \"prefix\": \"doc\"},\n",
    "        # customize fields that are indexed\n",
    "        \"fields\": [\n",
    "            # required fields for llamaindex\n",
    "            {\"type\": \"tag\", \"name\": \"id\"},\n",
    "            {\"type\": \"tag\", \"name\": \"doc_id\"},\n",
    "            {\"type\": \"text\", \"name\": \"text\"},\n",
    "            # custom vector field for bge-small-en-v1.5 embeddings\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"name\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"dims\": 384,\n",
    "                    \"algorithm\": \"hnsw\",\n",
    "                    \"distance_metric\": \"cosine\",\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "vector_store = RedisVectorStore(\n",
    "    schema=custom_schema,\n",
    "    redis_url=\"redis://default:uGTOv5VlEb1lTMwef3rcT6KppM73it2Q@redis-10097.c8.us-east-1-4.ec2.redns.redis-cloud.com:10097\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "350dd8af-d49f-40a3-a6e2-30b6333ff9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the ingestion cache layer\n",
    "cache = IngestionCache(\n",
    "    cache=RedisCache.from_host_and_port(\"localhost\", 6379),\n",
    "    collection=\"redis_cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd12738e-fe0a-47b2-93ea-07aabaa242aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        embed_model,\n",
    "    ],\n",
    "    docstore=RedisDocumentStore.from_host_and_port(\n",
    "        \"localhost\", 6379, namespace=\"document_store\"\n",
    "    ),\n",
    "    vector_store=vector_store,\n",
    "    cache=cache,\n",
    "    docstore_strategy=DocstoreStrategy.UPSERTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ac0d5-9498-4f9f-ba61-4999e4cebf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")  \n",
    "\n",
    "llm = Groq(model=\"llama3-70b-8192\", api_key=GROQ_API_KEY)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcd16738-62b3-4663-9580-cd549dc9d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    pipeline.vector_store, embed_model=embed_model\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15280b39-07f2-447d-8aff-677e10a2aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a41142e3-460e-4cd1-abfa-7447a6476191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the sub-types of question answering?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcb4e4ae-14ce-4ebc-968d-f76db17acab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"Data\")\n",
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31f6a8b6-6d6c-4fae-9b46-b4d2f8e8975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 13 Nodes\n"
     ]
    }
   ],
   "source": [
    "nodes = pipeline.run(documents=docs)\n",
    "print(f\"Ingested {len(nodes)} Nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39bbfd0f-f92c-4c86-ad11-1e3975c8c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A has all sorts of sub-types, such as: What to do - Semantic search: finding data that matches not just your query terms, but your intent and the meaning behind your question. This is sometimes known as \"top k\" search. - Summarization: condensing a large amount of data into a short summary relevant to your current question. Where to search - Over documents: LlamaIndex can pull in unstructured text, PDFs, Notion and Slack documents and more and index the data within them. - Over structured data: if your data already exists in a SQL database, as JSON or as any number of other structured formats, LlamaIndex can query the data in these sources. How to search - Combine multiple sources: is some of your data in Slack, some in PDFs, some in unstructured text? LlamaIndex can combine queries across an arbitrary number of sources and combine them. - Route across multiple sources: given multiple data sources, your application can first pick the best source and then \"route\" the question to that source. - Multi-document queries: some questions have partial answers in multiple data sources which need to be questioned separately before they can be combined.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the sub-types of question answering?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944164cb-584b-4bc7-8148-1d6b4713f25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba4cbb-62f7-475b-8a64-ea8106c138e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922a16e-25d7-4971-abc1-aa4b3750f6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f3e5f-a967-4d52-93db-4d386216a0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480445f-24e1-4b01-8175-b23ddf2ce201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced4a4c-2b56-4098-b2de-2292fe42f755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4729360c-c72f-4e59-a01b-85d7aabe86f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0216170-697f-4edc-9f1f-738e559acb13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca39ae0-d754-436b-9022-83ccc6a43b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
